{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Deep Deterministic Policy Gradient: DDPG\n",
    "This notebook is an implementation of the DDPG algorithm to solve the Reacher environment. You can find\n",
    "an explanation of DDPG in this [paper](https://arxiv.org/abs/1509.02971)\n",
    "and an explanation of the Reacher enviornment [here](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md).\n",
    "\n",
    "## 1. Import all necessary packages\n",
    "If you have any trouble importing these packages make sure you check the README file and have all the necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from agents import DDPG\n",
    "\n",
    "#-------- Setup Notebook Variables ---------#\n",
    "PATH = \"C:\\Dev\\Python\\RL\\\\Udacity_Continuous_Control\"\n",
    "train = True # True trains new agent, False uses already trained agent\n",
    "watch_untrained = True\n",
    "watch_trained = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Setup the Environment\n",
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python.\n",
    "**Note:** `file_name` parameter must match the location of the Unity environment that you downloaded."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='C:\\Dev\\Python\\RL\\\\Udacity_Continuous_Control\\Reacher_Windows_x86_64_multiple\\Reacher.exe', no_graphics=True)\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "# reset the environment and get env info to setup Agent\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "action_size = brain.vector_action_space_size\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Watch Untrained Agent\n",
    "This section shows you what the untrained agents looks like performing in the environment. If you do not want to watch\n",
    "the untrained agents set `watch_untrained = false` in section 1."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def watch_untrained(agent, env, brain_name):\n",
    "    # Setup\n",
    "    env_info = env.reset(train_mode=False)[brain_name]\n",
    "    states = env_info.vector_observations\n",
    "    # Run the simulation\n",
    "    for t in range(300):\n",
    "        actions = agent.act(states, agent.epsilon, add_noise=False)\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        next_states, dones = env_info.vector_observations, env_info.local_done\n",
    "        states = next_states\n",
    "        if np.any(dones):\n",
    "            break\n",
    "\n",
    "if watch_untrained:\n",
    "    agent = DDPG(state_size=state_size, action_size=action_size, num_agents=len(env_info.agents), epsilon=0, random_seed = 2)\n",
    "    watch_untrained(agent)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Setup and Train the Agent\n",
    "This section trains the agent. If you do not want to train the agent and only wish to see the result of training and view the agent then set\n",
    "`train = false` in section 1."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10 \tAverage Score: 0.31\n",
      "Episode: 20 \tAverage Score: 0.55\n",
      "Episode: 30 \tAverage Score: 1.14\n",
      "Episode: 40 \tAverage Score: 1.58\n",
      "Episode: 50 \tAverage Score: 2.26\n",
      "Episode: 60 \tAverage Score: 3.45\n",
      "Episode: 70 \tAverage Score: 5.03\n",
      "Episode: 80 \tAverage Score: 8.71\n",
      "Episode: 90 \tAverage Score: 13.44\n",
      "Episode: 100 \tAverage Score: 21.11\n",
      "Episode: 110 \tAverage Score: 27.24\n",
      "Episode: 120 \tAverage Score: 35.34\n"
     ]
    }
   ],
   "source": [
    "ddpg_agent = DDPG(state_size=state_size, action_size=action_size, num_agents=len(env_info.agents), epsilon=1, random_seed = 2)\n",
    "if train:\n",
    "    ddpg_agent.train(env, brain_name)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. View the Results of Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def graph_results(scores_filename, avgScores_filename, save_graph = True):\n",
    "    \"\"\" Graph results from training an Agent\n",
    "\n",
    "    :param filename: file to get scores from\n",
    "    \"\"\"\n",
    "    # Read in results from file\n",
    "    with open(scores_filename) as f:\n",
    "        scores = [round(float(score),2) for score in f.read().splitlines()]\n",
    "    with open(avgScores_filename) as f:\n",
    "        avg_scores = [round(float(score),2) for score in f.read().splitlines()]\n",
    "\n",
    "    # Graph results\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set(xlabel=\"Episode #\", ylabel=\"Score\", title=\"DDPG Agent Learning Curve\")\n",
    "    ax.grid()\n",
    "    ax.plot(np.arange(len(scores)), scores, label=\"DDPG Agent Episode Scores\")\n",
    "    ax.plot(np.arange(len(avg_scores)), avg_scores, label=\"DDPG Agent 100 Episode Average\")\n",
    "    ax.plot(np.arange(len(scores)), np.ones(len(scores))*30, color=\"black\", linestyle=\"dashed\", label=\"Solved\")\n",
    "    ax.legend()\n",
    "    # Save graph results\n",
    "    if save_graph:\n",
    "        fig.savefig(f'{PATH}\\images\\DDPG_Agent_Multiple.png')\n",
    "    plt.show()\n",
    "\n",
    "graph_results(f'{PATH}\\scores\\DDPG_Agent_Multiple_Scores.txt',\n",
    "              f'{PATH}\\scores\\DDPG_Agent_Multiple_AvgScores.txt')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Watch the Trained Agent\n",
    "This section shows you what the trained agents looks like performing in the environment. If you do not want to watch the\n",
    "trained agents set `watch_trained = false` in section 1."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def watch_trained(agent, env, brain_name):\n",
    "    # Setup\n",
    "    agent.actor_local.load_state_dict(torch.load('DDPG_Agent_Actor_Multiple.pth'))\n",
    "    agent.critic_local.load_state_dict(torch.load('DDPG_Agent_Critic_Multiple.pth'))\n",
    "    env_info = env.reset(train_mode=False)[brain_name]\n",
    "    states = env_info.vector_observations\n",
    "    # Run the simulation\n",
    "    for t in range(300):\n",
    "        actions = agent.act(states, agent.epsilon, add_noise=False)\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        next_states, dones = env_info.vector_observations, env_info.local_done\n",
    "        states = next_states\n",
    "        if np.any(dones):\n",
    "            break\n",
    "\n",
    "if watch_trained:\n",
    "    agent = DDPG(state_size=state_size, action_size=action_size, num_agents=len(env_info.agents), epsilon=0, random_seed = 2)\n",
    "    watch_trained(agent, env, brain_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}