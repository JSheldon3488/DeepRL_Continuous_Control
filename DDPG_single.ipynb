{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Deep Deterministic Policy Gradient: DDPG\n",
    "This notebook is an implementation of the DDPG algorithm to solve the Reacher environment. You can find\n",
    "an explanation of DDPG in this [paper](https://arxiv.org/abs/1509.02971)\n",
    "and an explanation of the Reacher enviornment [here](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md).\n",
    "\n",
    "## 1. Import all necessary packages\n",
    "If you have any trouble importing these packages make sure you check the README file and have all the necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from agents import DDPG\n",
    "\n",
    "PATH = \"C:\\Dev\\Python\\RL\\\\Udacity_Continuous_Control\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Setup the Environment\n",
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python.\n",
    "**Note:** `file_name` parameter must match the location of the Unity environment that you downloaded."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='C:\\Dev\\Python\\RL\\\\Udacity_Continuous_Control\\Reacher_Windows_x86_64_single\\Reacher.exe')\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "# reset the environment and get env info to setup Agent\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "action_size = brain.vector_action_space_size\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Setup and Train the Agent\n",
    "This section contains a function for training the agent. If you do not want to train the agent and only wish to see the result of training and view the agent then set\n",
    "`train = false` or skip this section."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10 \tAverage Score: 0.1\n",
      "Episode: 20 \tAverage Score: 0.15\n",
      "Episode: 30 \tAverage Score: 0.17\n",
      "Episode: 40 \tAverage Score: 0.64\n",
      "Episode: 50 \tAverage Score: 0.26\n",
      "Episode: 60 \tAverage Score: 0.43\n",
      "Episode: 70 \tAverage Score: 0.41\n",
      "Episode: 80 \tAverage Score: 0.36\n",
      "Episode: 90 \tAverage Score: 0.62\n",
      "Episode: 100 \tAverage Score: 0.43\n"
     ]
    }
   ],
   "source": [
    "ddpg_agent = DDPG(state_size = state_size, action_size = action_size, random_seed = 88)\n",
    "train = True\n",
    "\n",
    "def train_agent(agent: DDPG, num_episodes= 200, max_time = 300, print_every = 10):\n",
    "    scores = []\n",
    "    scores_deque = deque(maxlen=print_every)\n",
    "\n",
    "    # Simulations for all the episodes\n",
    "    for episode_num in range(1,num_episodes+1):\n",
    "        # Reset everything\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations[0]\n",
    "        score = 0\n",
    "        # Run the episode\n",
    "        for t in range(max_time):\n",
    "            action = agent.act(state)\n",
    "            env_info = env.step(action)[brain_name]\n",
    "            next_state, reward, done = env_info.vector_observations[0], env_info.rewards[0], env_info.local_done[0]\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            score += reward\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        # Episode Finished\n",
    "        scores.append(score)\n",
    "        scores_deque.append(score)\n",
    "        if episode_num % print_every == 0:\n",
    "            print(f'Episode: {episode_num} \\tAverage Score: {round(np.mean(scores_deque),2)}')\n",
    "            torch.save(agent.actor_local.state_dict(), f'{PATH}\\checkpoints\\{agent}_Actor_Single.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), f'{PATH}\\checkpoints\\{agent}_Critic_Single.pth')\n",
    "\n",
    "    # All Episodes finished Save parameters and scores\n",
    "    torch.save(agent.actor_local.state_dict(), f'{PATH}\\checkpoints\\{agent}_Actor_Single.pth')\n",
    "    torch.save(agent.critic_local.state_dict(), f'{PATH}\\checkpoints\\{agent}_Critic_Single.pth')\n",
    "    f = open(f'{PATH}\\scores\\{agent}_Single.txt', 'w')\n",
    "    scores_string = \"\\n\".join([str(score) for score in scores])\n",
    "    f.write(scores_string)\n",
    "    f.close\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "if train:\n",
    "    train_agent(ddpg_agent)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. View the Results of Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Dev\\\\Python\\\\RL\\\\Udacity_Continuous_Control\\\\scores\\\\DDPG_Agent_Single.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-5-4f0edcde0312>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     20\u001B[0m     \u001B[0mplt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 22\u001B[1;33m \u001B[0mgraph_results\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf'{PATH}\\scores\\DDPG_Agent_Single.txt'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     23\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-5-4f0edcde0312>\u001B[0m in \u001B[0;36mgraph_results\u001B[1;34m(filename, save_graph)\u001B[0m\n\u001B[0;32m      5\u001B[0m     \"\"\"\n\u001B[0;32m      6\u001B[0m     \u001B[1;31m# Read in results from file\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 7\u001B[1;33m     \u001B[1;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      8\u001B[0m         \u001B[0mscores\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mround\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfloat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mscore\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mscore\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msplitlines\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'C:\\\\Dev\\\\Python\\\\RL\\\\Udacity_Continuous_Control\\\\scores\\\\DDPG_Agent_Single.txt'"
     ]
    }
   ],
   "source": [
    "def graph_results(filename, save_graph = True):\n",
    "    \"\"\" Graph results from training an Agent\n",
    "\n",
    "    :param filename: file to get scores from\n",
    "    \"\"\"\n",
    "    # Read in results from file\n",
    "    with open(filename) as f:\n",
    "        scores = [round(float(score),2) for score in f.read().splitlines()]\n",
    "\n",
    "    # Graph results\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set(xlabel=\"Episode #\", ylabel=\"Score\", title=\"DDPG Agent Learning Curve\")\n",
    "    ax.grid()\n",
    "    ax.plot(np.arange(len(scores)), scores, label=\"DDPG Agent Scores\")\n",
    "    ax.plot(np.arange(len(scores)), np.ones(len(scores))*30, color=\"black\", linestyle=\"dashed\", label=\"Solved\")\n",
    "    ax.legend()\n",
    "    # Save graph results\n",
    "    if save_graph:\n",
    "        fig.savefig(f'{PATH}\\images\\DDPG_Agent_Single.png')\n",
    "    plt.show()\n",
    "\n",
    "graph_results(f'{PATH}\\scores\\DDPG_Agent_Single.txt')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Watch the Trained Agent"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}