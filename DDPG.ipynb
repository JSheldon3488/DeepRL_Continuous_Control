{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Deep Deterministic Policy Gradient: DDPG\n",
    "This notebook is an implementation of the DDPG algorithm to solve the Reacher environment. You can find\n",
    "an explanation of DDPG [here](https://arxiv.org/abs/1509.02971)\n",
    "and an explanation of the Reacher enviornment [here](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md).\n",
    "\n",
    "## 1. Import all necessary packages\n",
    "If you have any trouble importing these packages make sure you check the README file and have all the necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from agents import *\n",
    "\n",
    "PATH = \"C:\\Dev\\Python\\RL\\\\Udacity_Continuous_Control\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Setup the Environment\n",
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python.\n",
    "**Note:** `file_name` parameter must match the location of the Unity environment that you downloaded."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name='C:\\Dev\\Python\\RL\\\\Udacity_Continuous_Control\\Reacher_Windows_x86_64_single\\Reacher.exe')\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "# reset the environment and get env info to setup Agent\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "num_agents = len(env_info.agents)\n",
    "action_size = brain.vector_action_space_size\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Setup and Train the Agent\n",
    "This section contains a function for training the agent. If you do not want to train the agent and only wish to see the result of training and view the agent then set\n",
    "`train = false` or skip this section."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ddpg_agent = DDPG(num_agents = num_agents, state_size = state_size, action_size = action_size, random_seed = 88)\n",
    "train = True\n",
    "\n",
    "def train_agent(agent: Agent, num_episodes= 1000, max_time = 300, print_every = 100):\n",
    "    scores = []\n",
    "    scores_deque = deque(maxlen=print_every)\n",
    "\n",
    "    # Simulations for all the episodes\n",
    "    for episode_num in range(1,num_episodes+1):\n",
    "        # Reset everything\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations[0]\n",
    "        score = 0\n",
    "        # Run the episode\n",
    "        for t in max_time:\n",
    "            action = agent.act(state)\n",
    "            env_info = env.step(action)[brain_name]\n",
    "            next_state, reward, done = env_info.vector_observations[0], env_info.rewards[0], env_info.local_done[0]\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            score += reward\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        # Episode Finished\n",
    "        scores.append(score)\n",
    "        scores_deque.append(score)\n",
    "        if episode_num % print_every == 0:\n",
    "            print(f'r\\Episode: {episode_num} \\tAverage Score: {round(np.mean(scores_deque),2)}')\n",
    "            torch.save(agent.actor_local.state_dict(), f'{PATH}\\checkpoints\\{agent}_actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), f'{PATH}\\checkpoints\\{agent}_critic.pth')\n",
    "\n",
    "    # All Episodes finished Save parameters and scores\n",
    "    torch.save(agent.actor_local.state_dict(), f'{PATH}\\checkpoints\\{agent}_actor.pth')\n",
    "    torch.save(agent.critic_local.state_dict(), f'{PATH}\\checkpoints\\{agent}_critic.pth')\n",
    "    f = open(f'{PATH}\\scores\\{agent}.txt', 'w')\n",
    "    scores_string = \"\\n\".join([score for score in scores])\n",
    "    f.write(scores_string)\n",
    "    f.close\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "if train:\n",
    "    train_agent(ddpg_agent)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. View the Results of Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Watch the Trained Agent"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}