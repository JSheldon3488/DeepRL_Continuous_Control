{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Deep Deterministic Policy Gradient: DDPG\n",
    "This notebook is an implementation of the DDPG algorithm to solve the Unity Crawler environment. You can find\n",
    "an explanation of DDPG in this [paper](https://arxiv.org/abs/1509.02971)\n",
    "and an explanation of the Crawler environment [here](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md).\n",
    "\n",
    "## 1. Import/Setup all necessary packages and variables\n",
    "If you have any trouble importing these packages make sure you check the README file and have all the necessary dependencies."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "from agents import DDPG\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#-------- Setup Notebook Variables ---------#\n",
    "PATH = \"C:\\Dev\\Python\\RL\\\\Udacity_Continuous_Control\"\n",
    "train = True # True trains new agent, False uses already trained agent\n",
    "watch_untrained = True\n",
    "view_results = True\n",
    "watch_trained = True\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Setup the Environment\n",
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python.\n",
    "**Note:** `file_name` parameter must match the location of the Unity environment that you downloaded."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name='C:\\Dev\\Python\\RL\\\\Udacity_Continuous_Control\\Reacher_Windows_x86_64_multiple\\Reacher.exe')\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Watch Untrained Agent\n",
    "This section shows you what the untrained agents looks like performing in the environment. If you do not want to watch\n",
    "the untrained agents set `watch_untrained = false` in section 1."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def watch_untrained(agent, env, brain_name):\n",
    "    \"\"\" Watch an untrained agent interact in a specific environment\n",
    "\n",
    "    :param agent: The untrained agent\n",
    "    :param env: The Unity environment the agent takes actions in\n",
    "    :param brain_name: Brain of the environment\n",
    "    \"\"\"\n",
    "    # Setup\n",
    "    env_info = env.reset(train_mode=False)[brain_name]\n",
    "    states = env_info.vector_observations\n",
    "    # Run the simulation\n",
    "    for t in range(200):\n",
    "        actions = agent.act(states, agent.epsilon, add_noise=False)\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        next_states, dones = env_info.vector_observations, env_info.local_done\n",
    "        states = next_states\n",
    "        if np.any(dones):\n",
    "            break\n",
    "\n",
    "if watch_untrained:\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    agent = DDPG(state_size=brain.vector_observation_space_size, action_size=brain.vector_action_space_size, num_agents=len(env_info.agents), epsilon=0, random_seed = 2)\n",
    "    watch_untrained(agent, env, brain_name)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Setup and Train the Agent\n",
    "This section trains the agent. If you do not want to train the agent and only wish to see the result of training an agent and watch an already trained agent then set\n",
    "`train = false` in section 1."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if train:\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    ddpg_agent = DDPG(state_size=brain.vector_observation_space_size, action_size=brain.vector_action_space_size, num_agents=len(env_info.agents), epsilon=1, random_seed = 2)\n",
    "    ddpg_agent.train(env, brain_name)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. View the Results of Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def graph_results(scores_filename, avgScores_filename, save_graph = True):\n",
    "    \"\"\" Graph results from training an Agent\n",
    "\n",
    "    :param scores_filename: filepath with episode scores\n",
    "    :param avgScores_filename: filepath with average (per 100) episode scores\n",
    "    :param save_graph: Bool for saving or just viewing graph\n",
    "    \"\"\"\n",
    "    # Read in scores from the files\n",
    "    with open(scores_filename) as f:\n",
    "        scores = [round(float(score),2) for score in f.read().splitlines()]\n",
    "    with open(avgScores_filename) as f:\n",
    "        avg_scores = [round(float(score),2) for score in f.read().splitlines()]\n",
    "\n",
    "    # Graph results\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set(xlabel=\"Episode #\", ylabel=\"Episode Scores\", title=\"DDPG Agent Learning Curve\")\n",
    "    ax.grid()\n",
    "    ax.plot(np.arange(len(scores)), scores, label=\"Episode Scores\")\n",
    "    ax.plot(np.arange(len(avg_scores)), avg_scores, label=\"100 Episode Average\")\n",
    "    ax.plot(np.arange(len(scores)), np.ones(len(scores))*30, color=\"black\", linestyle=\"dashed\", label=\"Solved\")\n",
    "    ax.legend()\n",
    "    # Save graph results\n",
    "    if save_graph:\n",
    "        fig.savefig(f'{PATH}\\images\\DDPG_Agent_Multiple.png')\n",
    "    plt.show()\n",
    "\n",
    "if view_results:\n",
    "    graph_results(f'{PATH}\\scores\\DDPG_Agent_Multiple_Scores.txt',\n",
    "                  f'{PATH}\\scores\\DDPG_Agent_Multiple_AvgScores.txt')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Watch the Trained Agent\n",
    "This section shows you what the trained agents looks like performing in the environment. If you do not want to watch the\n",
    "trained agents set `watch_trained = false` in section 1."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def watch_trained(agent, env, brain_name):\n",
    "    \"\"\" Watch a trained agent interact in the environment. This agent will load the most recent checkpoints in checkpoints directory.\n",
    "\n",
    "    :param agent: Agent used to learn and interact in the environment\n",
    "    :param env: Unity environment for agent to act in\n",
    "    :param brain_name: Brain of environment\n",
    "    \"\"\"\n",
    "    # Load in agents trained parameters\n",
    "    agent.actor_local.load_state_dict(torch.load(f'{PATH}\\checkpoints\\DDPG_Agent_Actor_Multiple.pth'))\n",
    "    agent.critic_local.load_state_dict(torch.load(f'{PATH}\\checkpoints\\DDPG_Agent_Critic_Multiple.pth'))\n",
    "\n",
    "    # Watch the trained Agent\n",
    "    env_info = env.reset(train_mode=False)[brain_name]\n",
    "    states = env_info.vector_observations\n",
    "    for t in range(300):\n",
    "        actions = agent.act(states, agent.epsilon, add_noise=False)\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        next_states, dones = env_info.vector_observations, env_info.local_done\n",
    "        states = next_states\n",
    "        if np.any(dones):\n",
    "            break\n",
    "\n",
    "if watch_trained:\n",
    "    env_info = env.reset(train_mode=False)[brain_name]\n",
    "    agent = DDPG(state_size=brain.vector_observation_space_size, action_size=brain.vector_action_space_size, num_agents=len(env_info.agents), epsilon=0, random_seed = 2)\n",
    "    watch_trained(agent, env, brain_name)\n",
    "\n",
    "\n",
    "env.close()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}